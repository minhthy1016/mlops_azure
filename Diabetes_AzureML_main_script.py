# This file has been autogenerated by version 1.45.0 of the Azure Automated Machine Learning SDK.


import numpy
import numpy as np
import pandas as pd
import pickle
import argparse


from azureml.training.tabular._diagnostics import logging_utilities


def setup_instrumentation():
    import logging
    import sys

    from azureml.core import Run
    from azureml.telemetry import INSTRUMENTATION_KEY, get_telemetry_log_handler
    from azureml.telemetry._telemetry_formatter import ExceptionFormatter

    logger = logging.getLogger("azureml.training.tabular")

    try:
        logger.setLevel(logging.INFO)

        # Add logging to STDOUT
        stdout_handler = logging.StreamHandler(sys.stdout)
        logger.addHandler(stdout_handler)

        # Add telemetry logging with formatter to strip identifying info
        telemetry_handler = get_telemetry_log_handler(
            instrumentation_key=INSTRUMENTATION_KEY, component_name="azureml.training.tabular"
        )
        telemetry_handler.setFormatter(ExceptionFormatter())
        logger.addHandler(telemetry_handler)

        # Attach run IDs to logging info for correlation if running inside AzureML
        try:
            run = Run.get_context()
            parent_run = run.parent
            return logging.LoggerAdapter(logger, extra={
                "properties": {
                    "codegen_run_id": run.id,
                    "parent_run_id": parent_run.id
                }
            })
        except Exception:
            pass
    except Exception:
        pass

    return logger


logger = setup_instrumentation()


def split_dataset(X, y, weights, split_ratio, should_stratify):
    from sklearn.model_selection import train_test_split

    random_state = 42
    if should_stratify:
        stratify = y
    else:
        stratify = None

    if weights is not None:
        X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(
            X, y, weights, stratify=stratify, test_size=split_ratio, random_state=random_state
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, stratify=stratify, test_size=split_ratio, random_state=random_state
        )
        weights_train, weights_test = None, None

    return (X_train, y_train, weights_train), (X_test, y_test, weights_test)


def get_training_dataset(dataset_id):
    from azureml.core.dataset import Dataset
    from azureml.core.run import Run
    
    logger.info("Running get_training_dataset")
    ws = Run.get_context().experiment.workspace
    dataset = Dataset.get_by_id(workspace=ws, id=dataset_id)
    return dataset.to_pandas_dataframe()


def prepare_data(dataframe):
    from azureml.training.tabular.preprocessing import data_cleaning
    
    logger.info("Running prepare_data")
    label_column_name = 'Y'
    
    # extract the features, target and sample weight arrays
    y = dataframe[label_column_name].values
    X = dataframe.drop([label_column_name], axis=1)
    sample_weights = None
    X, y, sample_weights = data_cleaning._remove_nan_rows_in_X_y(X, y, sample_weights,
     is_timeseries=False, target_column=label_column_name)
    
    return X, y, sample_weights


def get_mapper_ab1045(column_names):
    from sklearn.impute import SimpleImputer
    from sklearn_pandas.dataframe_mapper import DataFrameMapper
    from sklearn_pandas.features_generator import gen_features
    
    definition = gen_features(
        columns=column_names,
        classes=[
            {
                'class': SimpleImputer,
                'add_indicator': False,
                'copy': True,
                'fill_value': None,
                'missing_values': numpy.nan,
                'strategy': 'mean',
                'verbose': 0,
            },
        ]
    )
    mapper = DataFrameMapper(features=definition, input_df=True, sparse=True)
    
    return mapper
    
    
def get_mapper_9133f9(column_names):
    from azureml.training.tabular.featurization.categorical.cat_imputer import CatImputer
    from azureml.training.tabular.featurization.categorical.labelencoder_transformer import LabelEncoderTransformer
    from azureml.training.tabular.featurization.text.stringcast_transformer import StringCastTransformer
    from sklearn_pandas.dataframe_mapper import DataFrameMapper
    from sklearn_pandas.features_generator import gen_features
    
    definition = gen_features(
        columns=column_names,
        classes=[
            {
                'class': CatImputer,
                'copy': True,
            },
            {
                'class': StringCastTransformer,
            },
            {
                'class': LabelEncoderTransformer,
                'hashing_seed_val': 314489979,
            },
        ]
    )
    mapper = DataFrameMapper(features=definition, input_df=True, sparse=True)
    
    return mapper
    
    
def generate_data_transformation_config():
    from sklearn.pipeline import FeatureUnion
    
    column_group_1 = ['SEX']
    
    column_group_2 = [['AGE'], ['BMI'], ['BP'], ['S1'], ['S2'], ['S3'], ['S4'], ['S5'], ['S6']]
    
    feature_union = FeatureUnion([
        ('mapper_ab1045', get_mapper_ab1045(column_group_2)),
        ('mapper_9133f9', get_mapper_9133f9(column_group_1)),
    ])
    return feature_union
    
    
def generate_preprocessor_config_0():
    from sklearn.preprocessing import StandardScaler
    
    preproc = StandardScaler(
        copy=True,
        with_mean=False,
        with_std=True
    )
    
    return preproc
    
    
def generate_algorithm_config_0():
    from sklearn.ensemble import ExtraTreesClassifier
    
    algorithm = ExtraTreesClassifier(
        bootstrap=False,
        ccp_alpha=0.0,
        class_weight='balanced',
        criterion='gini',
        max_depth=None,
        max_features=None,
        max_leaf_nodes=None,
        max_samples=None,
        min_impurity_decrease=0.0,
        min_impurity_split=None,
        min_samples_leaf=0.01,
        min_samples_split=0.056842105263157895,
        min_weight_fraction_leaf=0.0,
        n_estimators=200,
        n_jobs=-1,
        oob_score=False,
        random_state=None,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_1():
    from sklearn.preprocessing import RobustScaler
    
    preproc = RobustScaler(
        copy=True,
        quantile_range=[10, 90],
        with_centering=True,
        with_scaling=False
    )
    
    return preproc
    
    
def generate_algorithm_config_1():
    from sklearn.ensemble import ExtraTreesClassifier
    
    algorithm = ExtraTreesClassifier(
        bootstrap=False,
        ccp_alpha=0.0,
        class_weight='balanced',
        criterion='gini',
        max_depth=None,
        max_features=0.8,
        max_leaf_nodes=None,
        max_samples=None,
        min_impurity_decrease=0.0,
        min_impurity_split=None,
        min_samples_leaf=0.01,
        min_samples_split=0.33789473684210525,
        min_weight_fraction_leaf=0.0,
        n_estimators=25,
        n_jobs=-1,
        oob_score=False,
        random_state=None,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_2():
    from sklearn.preprocessing import MinMaxScaler
    
    preproc = MinMaxScaler(
        copy=True,
        feature_range=(0, 1)
    )
    
    return preproc
    
    
def generate_algorithm_config_2():
    from sklearn.ensemble import RandomForestClassifier
    
    algorithm = RandomForestClassifier(
        bootstrap=True,
        ccp_alpha=0.0,
        class_weight='balanced',
        criterion='gini',
        max_depth=None,
        max_features=0.2,
        max_leaf_nodes=None,
        max_samples=None,
        min_impurity_decrease=0.0,
        min_impurity_split=None,
        min_samples_leaf=0.01,
        min_samples_split=0.2442105263157895,
        min_weight_fraction_leaf=0.0,
        n_estimators=100,
        n_jobs=-1,
        oob_score=True,
        random_state=None,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_3():
    from sklearn.preprocessing import StandardScaler
    
    preproc = StandardScaler(
        copy=True,
        with_mean=False,
        with_std=False
    )
    
    return preproc
    
    
def generate_algorithm_config_3():
    from sklearn.ensemble import RandomForestClassifier
    
    algorithm = RandomForestClassifier(
        bootstrap=True,
        ccp_alpha=0.0,
        class_weight='balanced',
        criterion='entropy',
        max_depth=None,
        max_features=0.5,
        max_leaf_nodes=None,
        max_samples=None,
        min_impurity_decrease=0.0,
        min_impurity_split=None,
        min_samples_leaf=0.06157894736842105,
        min_samples_split=0.01,
        min_weight_fraction_leaf=0.0,
        n_estimators=25,
        n_jobs=-1,
        oob_score=False,
        random_state=None,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_4():
    from sklearn.preprocessing import MinMaxScaler
    
    preproc = MinMaxScaler(
        copy=True,
        feature_range=(0, 1)
    )
    
    return preproc
    
    
def generate_algorithm_config_4():
    from sklearn.ensemble import RandomForestClassifier
    
    algorithm = RandomForestClassifier(
        bootstrap=False,
        ccp_alpha=0.0,
        class_weight=None,
        criterion='gini',
        max_depth=None,
        max_features=0.5,
        max_leaf_nodes=None,
        max_samples=None,
        min_impurity_decrease=0.0,
        min_impurity_split=None,
        min_samples_leaf=0.06157894736842105,
        min_samples_split=0.01,
        min_weight_fraction_leaf=0.0,
        n_estimators=50,
        n_jobs=-1,
        oob_score=False,
        random_state=None,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_5():
    from sklearn.preprocessing import MinMaxScaler
    
    preproc = MinMaxScaler(
        copy=True,
        feature_range=(0, 1)
    )
    
    return preproc
    
    
def generate_algorithm_config_5():
    from sklearn.ensemble import ExtraTreesClassifier
    
    algorithm = ExtraTreesClassifier(
        bootstrap=False,
        ccp_alpha=0.0,
        class_weight='balanced',
        criterion='gini',
        max_depth=None,
        max_features=0.9,
        max_leaf_nodes=None,
        max_samples=None,
        min_impurity_decrease=0.0,
        min_impurity_split=None,
        min_samples_leaf=0.01,
        min_samples_split=0.01,
        min_weight_fraction_leaf=0.0,
        n_estimators=25,
        n_jobs=-1,
        oob_score=False,
        random_state=None,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_6():
    from sklearn.preprocessing import MinMaxScaler
    
    preproc = MinMaxScaler(
        copy=True,
        feature_range=(0, 1)
    )
    
    return preproc
    
    
def generate_algorithm_config_6():
    from sklearn.ensemble import ExtraTreesClassifier
    
    algorithm = ExtraTreesClassifier(
        bootstrap=True,
        ccp_alpha=0.0,
        class_weight=None,
        criterion='gini',
        max_depth=None,
        max_features=None,
        max_leaf_nodes=None,
        max_samples=None,
        min_impurity_decrease=0.0,
        min_impurity_split=None,
        min_samples_leaf=0.01,
        min_samples_split=0.056842105263157895,
        min_weight_fraction_leaf=0.0,
        n_estimators=50,
        n_jobs=-1,
        oob_score=False,
        random_state=None,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_7():
    from sklearn.preprocessing import MaxAbsScaler
    
    preproc = MaxAbsScaler(
        copy=True
    )
    
    return preproc
    
    
def generate_algorithm_config_7():
    from sklearn.ensemble import ExtraTreesClassifier
    
    algorithm = ExtraTreesClassifier(
        bootstrap=False,
        ccp_alpha=0.0,
        class_weight=None,
        criterion='gini',
        max_depth=None,
        max_features=0.7,
        max_leaf_nodes=None,
        max_samples=None,
        min_impurity_decrease=0.0,
        min_impurity_split=None,
        min_samples_leaf=0.035789473684210524,
        min_samples_split=0.01,
        min_weight_fraction_leaf=0.0,
        n_estimators=10,
        n_jobs=-1,
        oob_score=False,
        random_state=None,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_8():
    from sklearn.preprocessing import StandardScaler
    
    preproc = StandardScaler(
        copy=True,
        with_mean=True,
        with_std=True
    )
    
    return preproc
    
    
def generate_algorithm_config_8():
    from sklearn.linear_model import LogisticRegression
    
    algorithm = LogisticRegression(
        C=2222.996482526191,
        class_weight=None,
        dual=False,
        fit_intercept=True,
        intercept_scaling=1,
        l1_ratio=None,
        max_iter=100,
        multi_class='multinomial',
        n_jobs=-1,
        penalty='l2',
        random_state=None,
        solver='lbfgs',
        tol=0.0001,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_9():
    from sklearn.preprocessing import RobustScaler
    
    preproc = RobustScaler(
        copy=True,
        quantile_range=[10, 90],
        with_centering=False,
        with_scaling=True
    )
    
    return preproc
    
    
def generate_algorithm_config_9():
    from sklearn.svm import SVC
    
    algorithm = SVC(
        C=1048.1131341546852,
        break_ties=False,
        cache_size=200,
        class_weight='balanced',
        coef0=0.0,
        decision_function_shape='ovr',
        degree=3,
        gamma='scale',
        kernel='rbf',
        max_iter=-1,
        probability=True,
        random_state=None,
        shrinking=True,
        tol=0.001,
        verbose=False
    )
    
    return algorithm
    
    
def generate_preprocessor_config_10():
    from sklearn.preprocessing import MinMaxScaler
    
    preproc = MinMaxScaler(
        copy=True,
        feature_range=(0, 1)
    )
    
    return preproc
    
    
def generate_algorithm_config_10():
    from sklearn.ensemble import ExtraTreesClassifier
    
    algorithm = ExtraTreesClassifier(
        bootstrap=True,
        ccp_alpha=0.0,
        class_weight=None,
        criterion='entropy',
        max_depth=None,
        max_features=0.1,
        max_leaf_nodes=None,
        max_samples=None,
        min_impurity_decrease=0.0,
        min_impurity_split=None,
        min_samples_leaf=0.1131578947368421,
        min_samples_split=0.6657894736842105,
        min_weight_fraction_leaf=0.0,
        n_estimators=200,
        n_jobs=-1,
        oob_score=True,
        random_state=None,
        verbose=0,
        warm_start=False
    )
    
    return algorithm
    
    
def generate_algorithm_config():
    from azureml.automl.runtime.shared.model_wrappers import PreFittedSoftVotingClassifier
    from numpy import array
    from sklearn.pipeline import Pipeline
    
    pipeline_0 = Pipeline(steps=[('preproc', generate_preprocessor_config_0()), ('model', generate_algorithm_config_0())])
    pipeline_1 = Pipeline(steps=[('preproc', generate_preprocessor_config_1()), ('model', generate_algorithm_config_1())])
    pipeline_2 = Pipeline(steps=[('preproc', generate_preprocessor_config_2()), ('model', generate_algorithm_config_2())])
    pipeline_3 = Pipeline(steps=[('preproc', generate_preprocessor_config_3()), ('model', generate_algorithm_config_3())])
    pipeline_4 = Pipeline(steps=[('preproc', generate_preprocessor_config_4()), ('model', generate_algorithm_config_4())])
    pipeline_5 = Pipeline(steps=[('preproc', generate_preprocessor_config_5()), ('model', generate_algorithm_config_5())])
    pipeline_6 = Pipeline(steps=[('preproc', generate_preprocessor_config_6()), ('model', generate_algorithm_config_6())])
    pipeline_7 = Pipeline(steps=[('preproc', generate_preprocessor_config_7()), ('model', generate_algorithm_config_7())])
    pipeline_8 = Pipeline(steps=[('preproc', generate_preprocessor_config_8()), ('model', generate_algorithm_config_8())])
    pipeline_9 = Pipeline(steps=[('preproc', generate_preprocessor_config_9()), ('model', generate_algorithm_config_9())])
    pipeline_10 = Pipeline(steps=[('preproc', generate_preprocessor_config_10()), ('model', generate_algorithm_config_10())])
    algorithm = PreFittedSoftVotingClassifier(
        classification_labels=numpy.array([ 25,  31,  40,  42,  43,  47,  49,  50,  51,  53,  54,  55,  57,
                                      58,  59,  64,  65,  66,  67,  69,  70,  72,  73,  74,  75,  77,
                                      78,  79,  80,  81,  83,  84,  85,  86,  87,  88,  90,  91,  92,
                                      93,  94,  95,  96,  97,  99, 100, 102, 103, 104, 109, 111, 113,
                                     115, 116, 120, 121, 123, 124, 126, 127, 129, 131, 132, 134, 135,
                                     137, 138, 139, 141, 142, 143, 144, 147, 150, 151, 152, 154, 155,
                                     158, 160, 161, 162, 163, 168, 172, 173, 177, 178, 179, 180, 181,
                                     182, 183, 185, 190, 191, 192, 195, 196, 198, 199, 200, 201, 202,
                                     208, 209, 210, 212, 214, 215, 217, 221, 225, 229, 230, 233, 235,
                                     236, 241, 242, 243, 245, 246, 248, 249, 252, 257, 258, 259, 261,
                                     262, 263, 265, 268, 270, 272, 273, 274, 275, 277, 279, 281, 283,
                                     288, 292, 296, 302, 306, 310, 311, 317, 321, 332, 341]),
        estimators=[
            ('model_0', pipeline_0),
            ('model_1', pipeline_1),
            ('model_2', pipeline_2),
            ('model_3', pipeline_3),
            ('model_4', pipeline_4),
            ('model_5', pipeline_5),
            ('model_6', pipeline_6),
            ('model_7', pipeline_7),
            ('model_8', pipeline_8),
            ('model_9', pipeline_9),
            ('model_10', pipeline_10),
        ],
        flatten_transform=None,
        weights=[0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.2, 0.06666666666666667, 0.2, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667]
    )
    
    return algorithm
    
    
def build_model_pipeline():
    from sklearn.pipeline import Pipeline
    
    logger.info("Running build_model_pipeline")
    pipeline = Pipeline(
        steps=[
            ('featurization', generate_data_transformation_config()),
            ('ensemble', generate_algorithm_config()),
        ]
    )
    
    return pipeline


def train_model(X, y, sample_weights=None, transformer=None):
    logger.info("Running train_model")
    model_pipeline = build_model_pipeline()
    
    model = model_pipeline.fit(X, y)
    return model


def calculate_metrics(model, X, y, sample_weights, X_test, y_test, cv_splits=None):
    from azureml.training.tabular.score.scoring import score_classification
    
    y_pred_probs = model.predict_proba(X_test)
    if isinstance(y_pred_probs, pd.DataFrame):
        y_pred_probs = y_pred_probs.values
    class_labels = np.unique(y)
    train_labels = model.classes_
    metrics = score_classification(
        y_test, y_pred_probs, get_metrics_names(), class_labels, train_labels, use_binary=True)
    return metrics
def get_metrics_names():
    metrics_names = [
        'recall_score_classwise',
        'precision_score_macro',
        'accuracy_table',
        'average_precision_score_weighted',
        'precision_score_binary',
        'log_loss',
        'accuracy',
        'confusion_matrix',
        'AUC_micro',
        'matthews_correlation',
        'precision_score_weighted',
        'AUC_weighted',
        'iou_macro',
        'iou',
        'iou_weighted',
        'AUC_classwise',
        'balanced_accuracy',
        'weighted_accuracy',
        'precision_score_classwise',
        'recall_score_macro',
        'f1_score_binary',
        'recall_score_micro',
        'AUC_macro',
        'average_precision_score_binary',
        'f1_score_weighted',
        'iou_micro',
        'f1_score_macro',
        'average_precision_score_micro',
        'f1_score_classwise',
        'recall_score_binary',
        'average_precision_score_macro',
        'norm_macro_recall',
        'precision_score_micro',
        'classification_report',
        'recall_score_weighted',
        'average_precision_score_classwise',
        'iou_classwise',
        'f1_score_micro',
        'AUC_binary',
    ]
    return metrics_names


def main(training_dataset_id=None):
    from azureml.core.run import Run
    
    # The following code is for when running this code as part of an AzureML script run.
    run = Run.get_context()
    
    df = get_training_dataset(training_dataset_id)
    X, y, sample_weights = prepare_data(df)
    split_ratio = 0.3
    try:
        (X_train, y_train, sample_weights_train), (X_valid, y_valid, sample_weights_valid) = split_dataset(X, y, sample_weights, split_ratio, should_stratify=True)
    except Exception:
        (X_train, y_train, sample_weights_train), (X_valid, y_valid, sample_weights_valid) = split_dataset(X, y, sample_weights, split_ratio, should_stratify=False)
    model = train_model(X_train, y_train, sample_weights_train)
    
    metrics = calculate_metrics(model, X, y, sample_weights, X_test=X_valid, y_test=y_valid)
    
    print(metrics)
    for metric in metrics:
        run.log(metric, metrics[metric])
    
    with open('model.pkl', 'wb') as f:
        pickle.dump(model, f)
    run.upload_file('outputs/model.pkl', 'model.pkl')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--training_dataset_id', type=str, default='26505319-cac0-42d3-99c9-bb8c551c10be', help='Default training dataset id is populated from the parent run')
    args = parser.parse_args()
    
    try:
        main(args.training_dataset_id)
    except Exception as e:
        logging_utilities.log_traceback(e, logger)
        raise